---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
Hello, my name is Hamish. I recently completed masters in Statistics at UNSW. My thesis was on the application of self-supervised learning to cancer image classification. My undergraduate was in psychology and mathematics with a minor in logic and philosophy of science.

**Superintelligent AI is plausibly near**

Recently it has become clear that AI exceeding the human level is a matter of years or decades away. There are various ways this may pose an existential risk, with the most obvious being a mis-alignment between the AIs values and humans values, broadly construed. This can occur for at least two reasons: i) Through failure to specify the right goal, the paradigm example being a paperclip maximiser. ii) Through failure during training to learn the intended goal, and instead to learn a different objective not intended by the developers. Such a system is called a mesa-optimiser. A simple example is: a system is trained to reach the door in a maze. On the training distribution, all doors are red. On the test distribution, the doors are all blue, and there are other (non-door objects) that  are red. Although the performance at training time is ostensibly aligned, the alignment is spurious and at test time the system goes to red objects instead of doors. In other words, the AI learned the wrong objective. Specifying the correct goal as in i) Is called outer alignment. Ensuring the AI learns the intended goal during training as in ii) is called inner alignment. Both are unsolved problems at present. A potential solution to ii) is to ensure an AI is always truthful, although this also has subtle failure modes associated. For example an AI may be truthful at training but not test time. The inner/outer distinction is due to Hubinger et al. (2019).

